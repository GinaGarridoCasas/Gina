{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/\n",
    "\n",
    "En el anterior enlace, tenéis un ejemplo sobre cómo, a partir de tweets con un label específico (un sentimiento, positivo o negativo): \n",
    "\n",
    "1. Genera un conjunto de entrenamiento. El conjunto de entrenamiento es formado a partir de tweets completos pasados a un array con un tamaño específico.\n",
    "2. Ese array (X_train de tamaño N) tiene un label que representa el sentimiento (y_train)\n",
    "3. Como todas las frases tienen un tamaño N, la entrada de la red neuronal será de tamaño N y la salida de la red será de tamaño 2 usando activación softmax(porque hay dos clases).\n",
    "\n",
    "Se pide: \n",
    "\n",
    "- Realizar un clasificador de reviews para el dataset de IMDB de la carpeta data_exercise/\n",
    "\n",
    "**Cuando usa la importación \"keras.x\", reemplázalo por \"tensorflow.keras.x\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\"\"\"\n",
    "De\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "Usa\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import json\r\n",
    "from tensorflow import keras\r\n",
    "import tensorflow.keras.preprocessing.text as kpt\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"data_exercise/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training[\"review\"]\r\n",
    "y_train = training[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se asigna un valor numérico a las categorías de la variable y: sentiment (positivo:1, negativo:0)\r\n",
    "\r\n",
    "y_train = y_train.apply(lambda x: 1 if x == \"positive\" else 0)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se pasa a array la variable y_train\r\n",
    "\r\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 3000\r\n",
    "tokenizer = Tokenizer(num_words=max_words)  # Se divide el texto en 3000 palabras\r\n",
    "tokenizer.fit_on_texts(X_train) # Se aplica la tokenización al texto de entrada, de X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "dictionary = tokenizer.word_index\r\n",
    "\r\n",
    "with open('dictionary.json', 'w') as dictionary_file: \r\n",
    "    json.dump(dictionary, dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text): # pasar las palabras del texto a un array\r\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una lista con los índices de todas las palabras del texto\r\n",
    "\r\n",
    "allWordIndices = []\r\n",
    "\r\n",
    "for text in X_train:\r\n",
    "    wordIndices = convert_text_to_index_array(text)\r\n",
    "    allWordIndices.append(wordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "allWordIndices = np.asarray(allWordIndices) # Se pasa a array la lista de índices de las palabras del texto\r\n",
    "\r\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary') # Se pasa una matriz binaria el array de índices de las palabras del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se pasa categórica la variable y_train\r\n",
    "\r\n",
    "train_y = keras.utils.to_categorical(y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se establece el modelo de red neuronal \r\n",
    "\r\n",
    "model = Sequential()\r\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\r\n",
    "model.add(Dropout(0.5))\r\n",
    "model.add(Dense(256, activation='sigmoid'))\r\n",
    "model.add(Dropout(0.5))\r\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añaden los parámetros de compilación\r\n",
    "\r\n",
    "model.compile(loss='categorical_crossentropy',\r\n",
    "  optimizer='adam',\r\n",
    "  metrics= [\"CategoricalAccuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se fija el modelo\r\n",
    "\r\n",
    "hist_model = model.fit(train_x, train_y,\r\n",
    "  batch_size=32,\r\n",
    "  epochs=5,\r\n",
    "  verbose=1,\r\n",
    "  validation_split=0.1,\r\n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtiene la gráfica de la pérdida que se obtiene con el modelo (loss) en cada época, para los datos de entrenamiento y validación\r\n",
    "\r\n",
    "plt.plot(hist_model.history['loss'], label=' training data')\r\n",
    "plt.plot(hist_model.history['val_loss'], label='validation data)')\r\n",
    "plt.title('Loss for Review sentiment Classification')\r\n",
    "plt.ylabel('Loss value')\r\n",
    "plt.xlabel('No. epoch')\r\n",
    "plt.legend(loc=\"upper left\")\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtiene la gráfica de la precisión que se obtiene con el modelo (accuracy) en cada época, para los datos de entrenamiento y validación\r\n",
    "\r\n",
    "plt.plot(hist_model.history['categorical_accuracy'], label='training data')\r\n",
    "plt.plot(hist_model.history['val_categorical_accuracy'], label='validation data')\r\n",
    "plt.title('CategoricalAccuracy for Text Classification')\r\n",
    "plt.ylabel('CategoricalAccuracy value')\r\n",
    "plt.xlabel('No. epoch')\r\n",
    "plt.legend(loc=\"lower right\")\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se pasa el modelo a json y se archiva\r\n",
    "\r\n",
    "model_json = model.to_json()\r\n",
    "with open('model.json', 'w') as json_file:\r\n",
    "    json_file.write(model_json)\r\n",
    "\r\n",
    "model.save_weights('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5c4d2f1fdcd3716c7a5eea90ad07be30193490dd4e63617705244f5fd89ea793"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}